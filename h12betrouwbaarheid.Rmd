# Betrouwbaarheid {#ch:betrouwbaarheid}

## Inleiding

In Hoofdstuk \@ref(ch:validiteit)  hebben we het onder andere gehad over
construct-validiteit, de afstand tussen het bedoelde (theoretische)
concept of construct enerzijds, en de onafhankelijke of afhankelijke
variabele anderzijds. In dit hoofdstuk gaan we in op een ander zeer
belangrijk aspect van de afhankelijke variabele, nl. de
*betrouwbaarheid*. Deze betrouwbaarheid kan worden geschat op basis van
de samenhang of correlatie tussen observaties van hetzelfde construct.
We zullen ook ingaan op de relaties tussen betrouwbaarheid en
constructvaliditeit.

Vaak worden validiteit en betrouwbaarheid in één adem genoemd, en in opeenvolgende hoofdstukken besproken. Daar is wat voor te zeggen, want beide begrippen gaan over hoe je je variabelen definieert en operationaliseert. Toch hebben we hier gekozen voor een andere volgorde. Betrouwbaarheid komt pas aan bod nadat we samenhang besproken hebben (Hoofdstuk \@ref(ch:samenhang)), omdat betrouwbaarheid gebaseerd is op de samenhang of correlatie tussen observaties. 

## Wat is betrouwbaarheid?

Een betrouwbaar persoon is stabiel en voorspelbaar: wat hij of zij
vandaag doet is consistent met wat hij of zij vorige week deed, je kunt
op deze persoon vertrouwen --- in tegenstelling tot een onbetrouwbaar
persoon, die instabiel is en zich onvoorspelbaar gedraagt. Volgens *Van
Dale* is *betrouwbaarheid* "...de mate waarin iets of iem. te betrouwen
of geloofwaardig is". Betrouwbare metingen kunnen de basis vormen voor
een "justified true belief" (zie
§\@ref(sec:falsificatie)); onbetrouwbare metingen daarentegen zijn
per definitie niet waard om geloof aan te hechten.

Metingen vertonen altijd enige mate van fluctuatie of variatie of
inconsistentie. Die variabiliteit kan ten dele worden toegeschreven aan
de variatie in het gedrag dat gemeten wordt. Immers, zelfs als we
hetzelfde construct meten bij dezelfde persoon, dan nog zien we variatie
ten gevolge van de momentane mentale of fysieke toestand van de
proefpersoon, die nu eenmaal fluctueert. Bovendien is er variatie in het
gebruikte meet-instrument (thermometer, vragenlijst, sensor), en zijn er
wellicht inconsistenties in de wijze waarop wordt gemeten of beoordeeld.
Met de kwantificering van zulke consistenties en inconsistenties
betreden we het terrein van de betrouwbaarheidsanalyse.

Het begrip 'betrouwbaarheid' heeft in wetenschappelijk onderzoek
eigenlijk twee betekenissen, die we afzonderlijk behandelen. Ten eerste
wordt met betrouwbaarheid de *precisie* of *nauwkeurigheid* van een
meting bedoeld. Dit aspect heeft betrekking op de vraag in hoeverre de
meting beïnvloed wordt door toevallige factoren (waardoor de meting niet
uitsluitend het onderzochte construct weergeeft). Als we *niet*
nauwkeurig meten, dan weten we ook niet wat de verkregen meetwaarden
eigenlijk weergeven --- misschien het onderzochte construct, maar
misschien ook niet. Als we wel nauwkeurig meten, dan verwachten we, als
we dezelfde meting nogmaals zouden uitvoeren, dat we dan dezelfde
uitkomst zouden meten. Naarmate een meting minder precies is, zal er
meer variatie of inconsistentie zijn tussen de eerste meting en de
herhaalde meting, en zijn de metingen dus minder betrouwbaar.

---

> *Voorbeeld 12.1:* Als we de leesvaardigheid van
leerlingen in het eindexamen willen meten, dan leggen we hen een
tekstbegriptoets met een bijbehorend aantal vragen voor. De mate waarin
de verschillende vragen *hetzelfde* construct meten, hier het construct 'leesvaardigheid',
wordt de betrouwbaarheid, precisie of homogeniteit genoemd.

---


In het vervolg zullen we, om verwarring te voorkomen, deze vorm van
betrouwbaarheid dan ook aanduiden met de term *homogeniteit* (vs.
heterogeniteit). Bij een heterogene (niet-homogene) toets kan de
totaal-score moeilijk geïnterpreteerd worden. Bij een perfect homogene
test hebben mensen met dezelfde totaalscore ook dezelfde vragen correct
beantwoord. Maar als we menselijk (taal)gedrag meten, komen zulke
perfect homogene toetsen eigenlijk nooit voor: respondenten die wel
dezelfde totaalscore behalen, hebben toch niet allen dezelfde vragen
correct beantwoord (bv. in de leesvaardigheidstoets van het eindexamen,
voorbeeld 12.1). Dit houdt in dat de
vragen niet exact hetzelfde gemeten hebben. Dat is ook zo: de ene vraag
betrof een parafrase van een alinea, terwijl een andere vraag betrekking
had op een verwijswoord-antecedent-relatie. De vragen of items waren dus
niet perfect homogeen!

Ten tweede wordt met betrouwbaarheid de *stabiliteit* van een meting
bedoeld. Om je gewicht te meten ga je op een weegschaal staan. Die
meting is stabiel: vijf minuten later zal dezelfde weegschaal met
dezelfde persoon onder dezelfde omstandigheden ook (bijna) dezelfde meetwaarde
opleveren. De stabiliteit wordt veelal uitgedrukt in een zogenaamde
correlatiecoëfficiënt (een maat voor samenhang, zie Hoofdstuk
\@ref(ch:samenhang)). Deze correlatiecoëfficiënt kan alle waarden
aannemen tussen $+1$ en $-1$. Hoe meer de eerste en tweede meting gelijk
zijn, des te hoger de samenhang, en des te hoger de correlatie tussen
eerste en tweede meting. Omgekeerd, hoe lager de samenhang tussen eerste
en tweede meting, des te lager ook de correlatie.

Stabiele metingen komen echter zelden voor in onderzoek naar
(taal)gedrag. Als een toets tweemaal afgenomen wordt, dan is er vaak een
aanzienlijk verschil in scores op het eerste meetmoment en scores op het
tweede meetmoment.

---

> *Voorbeeld 12.2:*
Bij het eindexamen Nederlands moeten leerlingen een opstel schrijven,
dat wordt beoordeeld door twee beoordelaars. De beoordelaars zijn
stabiel, als zij na enige tijd dezelfde oordelen toekennen aan dezelfde
opstellen. Dus: als beoordelaar A eerst een 8 geeft aan een opstel, en
bij een tweede beoordeling enige tijd later ook een 8 geeft voor
hetzelfde opstel, dan is deze beoordelaar (zeer) stabiel. Als dezelfde
beoordelaar A bij de tweede beoordeling echter een 4 geeft aan ditzelfde
opstel, dan is de beoordelaar niet stabiel in zijn oordelen.

> Nu is het beoordelen van opstellen een lastige taak: de criteria zijn
niet scherp beschreven en er is relatief veel ruimte voor
interpretatie-verschillen. De stabiliteit van de oordelen is dan ook
laag; in het verleden is zelfs een stabiliteitscoëfficiënt van $0.40$
gerapporteerd.

---

Om de stabiliteit van een toets te berekenen, moet dezelfde toets
tweemaal worden afgenomen; de mate van samenhang tussen de eerste en
tweede meting wordt de *toets-hertoets-betrouwbaarheid* genoemd. Zo'n
herhaalde afname van een toets gebeurt in de praktijk zelden, vanwege de
relatief hoge kosten en relatief geringe opbrengst.

---

> *Voorbeeld 12.3:* 
[@Lata09] ontwikkelden
een Spaanstalige vragenlijst van 39 vragen, bedoeld voor
afasie-patiënten om de kwaliteit van hun leven te bepalen. De kwaliteit
van leven wordt omschreven als "the patient perception about, either the
effects of a given disease, or the application of a certain treatment on
different aspects of life, especially regarding its consequences on the
physical, emotional and social welfare" [@Lata09 p.379]. De nieuwe
vragenlijst werd tweemaal afgenomen bij een steekproef van 23
Spaanstalige patiënten met afasie t.g.v. hersenbloeding. De
gerapporteerde test-hertest-stabiliteit voor deze vragenlijst was
$0.95$.

---

Zowel de homogeniteit als stabiliteit wordt uitgedrukt als een
coëfficient met een waarde tussen $0$ en $1$ (negatieve coëfficienten
komen in de praktijk niet voor). Hoe moeten we de gerapporteerde
coëfficienten nu interpreteren? In het algemeen geldt natuurlijk: hoe
hoger de coëfficient, hoe hoger (beter) de betrouwbaarheid. Maar hoe
groot moet de betrouwbaarheid minimaal zijn voordat we een toets
"betrouwbaar" mogen noemen? Daar zijn geen eenduidige regels voor. Als
er afwegingen over personen gemaakt moeten worden, dan moet de toets een
betrouwbaarheid van minimaal $0.90$ hebben, volgens het Nederlands
Instituut van Psychologen (NIP). Dat geldt bijvoorbeeld voor toetsen die
worden gebruikt om te bepalen of een kind wel of niet in aanmerking komt
voor een zgn. dyslexie-verklaring. Voor onderzoeksdoeleinden hoeft niet
zo'n strenge eis aan de betrouwbaarheid van een toets te worden gesteld.
Vaak wordt als ondergrens voor de betrouwbaarheidscoëfficient de waarde
$0.70$ gehanteerd.

## Testtheorie

De klassieke testtheorie heeft betrekking op de meting van variabele $x$
bij het $i$-de element van een steekproef van willekeurige leden uit een
populatie. De testtheorie poneert dat elke meting $x_i$ samengesteld is
uit twee componenten, nl een ware score $t_i$ ('true score') en een
foutscore of afwijking $e_i$ ('error score'):
\begin{equation}
  x_i = t_i + e_i
  (\#eq:obs-true-error)
\end{equation}
Stel dat je "in het echt" $t=72.0$ kg weegt, en stel
dat je gemeten gewicht $x=71.6$ kg is, dan bedraagt de foutscore
$e=-0.4$ kg.

Een eerste belangrijke aanname in de klassieke testtheorie is dat de
afwijkingen $e_i$ elkaar neutraliseren of opheffen (d.w.z. gemiddeld nul
zijn, en dus niet systematisch afwijken van de ware score $t$), en dat
grotere afwijkingen naar boven of beneden minder vaker voorkomen dan
kleinere afwijkingen. Dat houdt in dat wordt aangenomen dat de
afwijkingen normaal verdeeld zijn (zie 
§\@ref(sec:normaalverdeling)), met $\mu_e=0$ als gemiddelde:
\begin{equation}
  (\#eq:normal-error)
  e_i \sim \mathcal{N}(0,s^2_e)
\end{equation}

Een tweede belangrijke aanname in de klassieke testtheorie is dat er
geen verband is tussen de ware scores $t_i$ en de foutscores $e_i$.
Omdat de component $e_i$ geheel door het toeval wordt bepaald, dus
zonder enig verband met $x_i$, is de correlatie tussen de ware score en
de foutscore nul:
\begin{equation}
  (\#eq:r-true-error)
  r_{(t,e)} = 0
\end{equation}

De totale variantie van $x$ is daarom[^fn12-1] gelijk aan som van de
variantie van de ware scores en de variantie van de foutscores:
\begin{equation}
  (\#eq:var-true-error)
  s^2_x = s^2_t + s^2_e
\end{equation}

Als de geobserveerde variantie $s^2_x$ verhoudingsgewijs veel
fout-variantie bevat (d.i. variantie van afwijkingen), dan zijn de
geobserveerde scores goeddeels bepaald door toevallige afwijkingen. Dat
is natuurlijk geen wenselijke zaak. We zeggen dan dat de geobserveerde
scores niet betrouwbaar zijn; er zit veel "ruis" in de geobserveerde
scores. Als de fout-variantie daarentegen relatief gering is, dan geven
de geobserveerde scores goed weer wat de ware scores zijn, en dan zijn
de geobserveerde verschillen wel goed betrouwbaar, d.w.z. ze worden
weinig bepaald door toevallige afwijkingen.

We kunnen de betrouwbaarheid (symbool $\rho$) dan ook definiëren als de
verhouding tussen ware-score-variantie en totale variantie:
\begin{equation}
  (\#eq:rho-betrouwbaarheid)
  \rho_{xx} = \frac{s^2_t}{s^2_x}
\end{equation}

We kunnen deze formule \@ref(eq:rho-betrouwbaarheid) echter in de praktijk niet gebruiken
om de betrouwbaarheid vast te stellen, omdat we $s^2_t$ niet kennen. We
moeten dus eerst schatten wat de ware-score-variantie is --- of wat de
fout-variantie is, die is immers het complement van de
ware-score-variantie (zie
formule \@ref(eq:var-true-error))[^fn12-2].

De tweede aanname (in formule \@ref(eq:r-true-error)) dat er geen verband is tussen de ware
score en de fout-score, is in de praktijk niet altijd gerechtvaardigd.
Dat wordt misschien inzichtelijk als we kijken naar de uitslagen van een
toets, op een schaal van 1.0 tot 10.0. De studenten met scores van 9 of
10 hebben ook een hoge ware score (zij beheersen de stof zeer goed) en
dus doorgaans een geringe fout-score. De studenten met scores van 1 of 2
hebben ook een lage ware score (zij beheersen de stof zeer slecht) en
dus ook doorgaans een geringe fout-score. Voor de studenten met scores
van 5 of 6 ligt de situatie anders: misschien beheersen ze de stof
redelijk goed maar hebben ze net een fout antwoord gegeven, of ze
beheersen de stof niet goed maar ze hebben toevallig een goed antwoord
gegeven. Voor deze studenten met een geobserveerde score in het midden
van de schaal zijn de fout-scores relatief groter dan voor de studenten
met een score bij de uiteinden van de schaal. In andere domeinen, bv bij
reactietijden, zien we andere verbanden, bv dat de foutscore min of meer
evenredig toeneemt met de score zelf; er is dan een positief verband
tussen de ware score en de foutscore ($\rho_{(t,e)}>0$). Desalniettemin
zijn de voordelen van de klassieke testtheorie zo groot dat we deze
theorie handhaven als uitgangspunt.

Uit de formules \@ref(eq:var-true-error) en \@ref(eq:rho-betrouwbaarheid) 
hierboven volgt ook dat de standaard
meetfout ('standard error of measurement', ) gerelateerd is aan de
standaarddeviatie en aan de betrouwbaarheid:
\begin{equation}
   (\#eq:standaard-meetfout)
    s_e = s_x \sqrt{1-r_{xx}}
\end{equation}    
Deze standaard meetfout is op te vatten
als de standaarddeviatie van de foutscores $e_i$, nog steeds aannemende
dat de foutscores normaal verdeeld zijn (formule \@ref(eq:normal-error)).

---

> *Voorbeeld 12.4:* 
Externe inspecteurs betwijfelen of docenten de eindwerkstukken van hun
studenten wel goed beoordelen. Als een student een 6 heeft gekregen, zou
dat werkstuk dan misschien eigenlijk als onvoldoende beoordeeld moeten
worden?

>  Laten we aannemen dat de gegeven oordelen een standaarddeviatie
$s_x=0.75$ vertonen, en laten we eveneens aannemen dat een analyse van
de betrouwbaarheid heeft laten zien dat $r_{xx}=0.9$. De standaard
meetfout bedraagt dan $s_e = 0.24$ punt (naar boven afgerond). De kans
dat de ware score $t_i$ kleiner of gelijk is aan 5.4 (onvoldoende), bij
een geobserveerde score van $x_i=6.0$ en $s_e=0.24$, is slechts $p=0.006$
(voor uitleg, zie §\@ref(sec:t-betrouwbaarheidsinterval-gemiddelde) hierna). 
De voldoende beoordeling van het eindwerkstuk is met grote
waarschijnlijkheid juist.

---

## Interpretaties

Voordat we ingaan op verschillende berekeningswijzen van de
betrouwbaarheid is het zinvol om stil te staan bij verschillende
interpretaties van betrouwbaarheidsschattingen.

Ten eerste kan de betrouwbaarheid geïnterpreteerd worden als de
proportie ware-score-variantie (zie formule \@ref(eq:rho-betrouwbaarheid)), 
oftewel als de proportie van
variantie die "systematisch" is. Dit is vanzelfsprekend nog lang niet
hetzelfde als de proportie variantie ten gevolge van het
begrip-zoals-bedoeld, de "valide" variantie (zie
Hoofdstuk \@ref(ch:validiteit)). De variantie ten gevolge van het
begrip-zoals-bedoeld maakt deel uit van de proportie
ware-score-variantie. Echter, tal van andere factoren kunnen op
systematische wijze van invloed zijn op de scores van de respondenten,
zoals verschillen in toets-ervaring. Indien twee studenten $i$ en $j$
een concept (zeg: taalvaardigheid) in dezelfde mate bezitten, dan nog
kan de ene student beter scoren omdat hij of zij vaker
taalvaardigheidstoetsen heeft afgelegd dan de andere student. Er is dan
geen verschil in het begrip zoals bedoeld (taalvaardigheid $T_i = T_j$),
maar wel in een andere factor (ervaring), en daardoor ontstaat een
verschil tussen de studenten in hun 'ware' scores ($t_i \neq t_j$) die
we meten met een valide en betrouwbare taalvaardigheidsmeting. Bij die
meting treden afwijkingen en meetfouten op ($e_i$ en $e_j$), waardoor de
geobserveerde verschillen tussen de studenten ($x_i-x_j$) groter of
kleiner kunnen zijn dan hun verschillen in 'ware' score ($t_i-t_j$).
Vandaar dat een betrouwbaarheidsschatting altijd de bovengrens vormt van
de validiteit.

Een tweede interpretatie van de betrouwbaarheid (formule \@ref(eq:rho-betrouwbaarheid)) 
is die van de theoretisch te verwachten correlatie (zie
§\@ref(sec:Pearson)) tussen de metingen, bij vele herhalingen van
die metingen. Gemakshalve nemen we aan dat geheugen- en
vermoeidheidseffecten geen enkel effect hebben bij de tweede en latere
metingen. Als we dezelfde personen met hetzelfde instrument driemaal
zouden meten, zonder effecten van geheugen of vermoeidheid, dan zouden
de scores van de eerste en tweede meting, en van de eerste en derde
meting, en van de tweede en derde meting, steeds dezelfde correlatie
$\rho$ vertonen. Die correlatie geeft dus aan in hoeverre de herhaalde
metingen consistent zijn, d.w.z. dezelfde onbekende ware score
representeren.

In deze interpretatie drukt de betrouwbaarheid dus de verwachte
samenhang uit tussen scores bij herhaalde afname van dezelfde toets. De
betrouwbaarheidscoëfficiënt $\rho$ interpreteren we dan als de
correlatie tussen twee metingen met hetzelfde instrument.

Ten derde kan de betrouwbaarheid geïnterpreteerd worden als het verlies
van efficiëntie in het schatten van de gemiddelde score $\overline{X}$
[@Ferg89 p.474]. Stel dat we de gemiddelde score van een groep van
$n=50$ proefpersonen willen vaststellen, en we gebruiken daarbij een
meetinstrument met betrouwbaarheid $\rho_{xx}=0.8$. Er is dan
onzekerheid in de schatting, deels afkomstig van variatie in de ware
scores $t_i$ van de proefpersonen, maar ook deels afkomstig van de
toevallige afwijkingen $e_i$ bij de metingen. Als het meetinstrument
perfect betrouwbaar zou zijn ($\rho=1$) dan zouden we slechts
$\rho_{xx}\times n = 0.8\times50=40$ proefpersonen nodig hebben gehad
voor dezelfde nauwkeurigheid van de schatting van $\overline{X}$
[@Ferg89 p.474]. We hebben dus als het ware 10 proefpersonen verspeeld
om te compenseren voor de onbetrouwbaarheid van het meetinstrument.

Hierboven hebben we gesproken over metingen met behulp van
meet-instrumenten, en hieronder zullen we spreken over beoordelingen
gedaan door beoordelaars. De benadering van de notie 'betrouwbaarheid'
is in deze situaties steeds gelijk. Betrouwbaarheid speelt een rol in
alle situaties waarin elementen uit een steekproef worden gemeten of
beoordeeld door meerdere beoordelaars of instrumenten. Ook tentamina en
vragenlijsten kunnen zulke meetinstrumenten zijn: een tentamen of een
vragenlijst is goed te beschouwen als een samengesteld instrument
waarmee we een abstracte eigenschap of construct van de deelnemers
proberen te meten. Iedere vraag is dan te beschouwen als een
"meetinstrument" of "beoordelaar" van de eigenschap of gesteldheid van
de respondent. Alle bovenvermelde inzichten en interpretaties aangaande
testtheorie, meetfout en betrouwbaarheid zijn daarbij evengoed van
toepassing.

## Methoden om betrouwbaarheid te schatten

De betrouwbaarheid van een meting kan op verschillende manieren worden
bepaald. De belangrijkste zijn de volgende:

-   De *toets-hertoets-methode*\
    We voeren alle metingen tweemaal uit, en berekenen daarna de
    correlatie tussen de eerste en de tweede meting. Naarmate de
    metingen minder meetfouten en afwijkingen bevatten, is de correlatie
    hoger en dus ook de betrouwbaarheid hoger. Deze methode is
    tijdrovend, maar kan ook worden toegepast op een kleine portie van
    de metingen. In spraak-onderzoek wordt deze methode wel gebruikt om
    de betrouwbaarheid van fonetische transcripties vast te stellen: een
    deel van de spraak-opnamen wordt door een tweede beoordelaar
    getranscribeerd, en vervolgens worden de beide transcripties
    vergeleken.

-   De *parallelle-toetsvorm-methode*\
    We hebben een grote verzameling meet-instrumenten die goed
    vergelijkbaar zijn en hetzelfde construct meten. We voeren alle
    metingen herhaaldelijk uit, de eerste keer door de metingen van
    enkele willekeurig getrokken meet-instrumenten te combineren (zeg A
    en B en C) en tweede keer met gebruik van andere willekeurige
    instrumenten (zeg D en E en F). Omdat de meetinstrumenten 'parallel'
    zijn en hetzelfde construct worden geacht te meten, is de correlatie
    tussen de eerste en de tweede meting een indicatie voor de
    betrouwbaarheid van de meting.

-   De *split-half-methode*\
    Deze methode lijkt op de parallelle-toetsvorm-methode. De $k$ vragen
    of instrumenten worden in twee helften verdeeld, waarna de score
    bepaald wordt binnen elke helft. Uit de correlatie $r_{hh}$ tussen
    de scores op de twee halve toetsen kan de betrouwbaarheid van de
    gehele toets afgeleid worden, $r_{xx} = \frac{2r_{hh}}{1+r_{hh}}$.

## Betrouwbaarheid tussen beoordelaars

Laten we als voorbeeld eens kijken naar metingen van de
spreekvaardigheid van studenten in een vreemde taal. Dit construct
'spreekvaardigheid' wordt in dit voorbeeld gemeten door middel van twee
beoordelaars, die onafhankelijk van elkaar elk een cijfer tussen 1 en
100 geven aan de student (hoger is beter). Bij de beoordeling treden
echter ook meetfouten op, waardoor de oordelen niet alleen de
onderliggende ware score weergeven, maar ook een afwijking daarvan, met
alle hierboven genoemde aannames. Laten we eerst alleen kijken naar de
oordelen door de eerste en tweede beoordelaar (zie
Tabel \@ref(tab:betrouwbaarheid)). Het eindoordeel over een student is
vooralsnog het gemiddelde van de oordelen van de eerste en tweede
beoordelaar.


Table: (#tab:betrouwbaarheid) Oordelen over spreekvaardigheid van $n = 10$ studenten (rijen) door
$k = 3$ beoordelaars (kolommen).

 student           B1    B2    B3
---------------   ----- ----- -----
    1              67    64    70 
    2              72    75    74 
    3              83    89    73
    4              68    72    61 
    5              75    72    77 
    6              73    73    78 
    7              73    76    72 
    8              65    73    72   
    9              64    68    71   
   10              70    82    69 
$\overline{x_i}$  71.0   74.4  71.7
$s_i$              5.6    7.0   4.7

De oordelen van alleen de eerste en de tweede beoordelaar vertonen een
onderlinge correlatie van $r_{1,2}=.75$. Dit houdt in (volgens
formule \@ref(eq:rho-betrouwbaarheid)) dat 75% van de totale variantie in
de oordelen van deze twee beoordelaars toe te schrijven is aan
verschillen tussen de beoordeelde studenten, en dus 25% aan meetfouten
(we hebben immers aangenomen dat er geen systematische verschillen zijn
tussen beoordelaars). Het aandeel van de meetfouten lijkt nogal hoog. We
mogen echter wel hoop putten uit één van onze eerdere aannames, nl dat
de meetfouten van de beoordelaars niet gecorreleerd zijn. De
*combinatie* van de twee beoordelaars --- de gemiddelde score per
student over de twee beoordelaars --- geeft dus beter betrouwbare
metingen dan elk van de beoordelaars afzonderlijk kunnen doen. Immers,
de meetfouten van de twee beoordelaars hebben de neiging om elkaar op te
heffen (zie formule \@ref(eq:normal-error)). 
Lees de laatste twee zinnen nog eens aandachtig door.

De betrouwbaarheid wordt vaak uitgedrukt als *Cronbach's Alpha*
[@Cort93]. Dat getal is een maat voor de consistentie of homogeniteit
van de metingen, en het geeft dus ook de mate aan waarin de twee
beoordelaars hetzelfde construct hebben beoordeeld. De eenvoudigste
definitie is gebaseerd op $\overline{r}$, de gemiddelde correlatie
tussen metingen van $k$ verschillende beoordelaars[^fn12-3].
\begin{equation}
  (\#eq:cronbach-corr)
  \alpha = \frac{k \overline{r}} {1+(k-1)\overline{r}}
\end{equation}
Invullen van $k=2$ beoordelaars en $\overline{r}=0.75$ geeft $\alpha=0.86$ (SPSS en R
gebruiken hiervoor een wat complexere formule, en rapporteren
$\alpha=0.84$). Deze maat voor betrouwbaarheid wordt niet alleen
aangeduid als Cronbach's Alpha, maar ook als de Spearman-Brown-formule
of als Kuder-Richardson-formule 20 (KR20)[^fn12-4].

De gevonden waarde van Cronbach's Alpha is wat lastig te evalueren,
omdat die mede afhankelijk is van het aantal instrumenten of
beoordelaars of vragen in de toets [@Cort93; @Glin01]. Voor
wetenschappelijk onderzoek wordt vaak een ondergrens van 0.75 of 0.80
gehanteerd. Indien de uitslag van de toets of meting van groot belang is
voor de betrokkene, zoals bij medische of psychische diagnostiek van
patiënten, of bij werving en selectie van personeel, dan wordt een nog
hogere betrouwbaarheid van $\alpha=.9$ aanbevolen [@Glin01].

Als we de betrouwbaarheid willen verhogen naar $\alpha=0.9$ of hoger,
dan kunnen we dat op twee manieren bereiken. De eerste manier is door
het aantal beoordelaars uit te breiden. Als we meer beoordelaars
combineren in de totale score, dan heffen de meetfouten van die
beoordelaars elkaar ook beter op, en dan wordt de totale score dus
betrouwbaarder. Met behulp van
formule \@ref(eq:cronbach-corr) kunnen we onderzoeken hoeveel beoordelaars
nodig zouden zijn, om de betrouwbaarheid te verbeteren naar
$\alpha=0.90$ of beter. We vullen $\alpha=0.90$ in en wederom
$\overline{r}=0.75$, en vinden dan een uitkomst van minimaal $k=3$
beoordelaars. De *toename* in de betrouwbaarheid vlakt af naarmate er al
meer beoordelaars meedoen: als $k=2$ dan $\alpha=.84$, als $k=3$ dan
$\alpha=.84+.06=.90$, als $k=6$ dan $\alpha=.90+.05=.95$, als $k=9$ dan
$\alpha=.95+.01=.96$, enz. Immers, als er al 6 beoordelaars zijn, die
elkaars meetfouten al goed opheffen, dan voegen 3 extra beoordelaars
weinig meer toe aan de betrouwbaarheid.

De tweede manier om de betrouwbaarheid te verhogen is om de meetfout te
verkleinen. Dat kunnen we proberen, bijvoorbeeld, door de beoordelaars
zo goed mogelijk te instrueren over hoe ze de spreekvaardigheid van de
studenten dienen te beoordelen. Een beoordelingsprotocol en/of
instructie kan de afwijkingen tussen en binnen beoordelaars doen
verkleinen. Kleinere afwijkingen betekenen kleinere meetfouten, en dat
betekent weer hogere correlaties tussen de beoordelaars. Bij een
$\overline{r}=0.8$ bereiken we al bijna de gewenste betrouwbaarheid, met
slechts $k=2$ beoordelaars.

Een derde manier om de betrouwbaarheid te verhogen vereist een nadere
analyse van de afzonderlijke beoordelaars. Om dit uit te leggen,
betrekken we nu ook de derde beoordelaar in onze beschouwingen (zie
Tabel \@ref(tab:betrouwbaarheid)). De oordelen van deze derde
beoordelaar vertonen echter lage correlaties met die van de eerste en
tweede beoordelaar: $r_{1,3}=0.41$ en $r_{2,3}=0.09$. Dat leidt ertoe
dat de gemiddelde correlatie tussen beoordelaars nu lager is,
$\overline{r}=0.42$. Door deze derde beoordelaar op te nemen, is de
betrouwbaarheid niet gestegen maar daarentegen juist gedaald naar
$\alpha = \frac{3\times0.42}{1+2\times0.42} = 0.68$. We doen er dus
misschien beter aan om de metingen van de derde beoordelaar te negeren.
Ook als we de betrouwbaarheid van een tentamen of toets of vragenlijst
onderzoeken, kan blijken dat de betrouwbaarheid van de gehele toets
*toeneemt* als we sommige "slechte" vragen verwijderen. Blijkbaar hebben
die "slechte" vragen een construct gemeten dat verschilt van wat de
resterende vragen gemeten hebben.

## Betrouwbaarheid en constructvaliditeit

Als een meting betrouwbaar is, dan is er "iets" betrouwbaar gemeten.
Maar let op: dat toont nog niet aan *wat* er gemeten is! Er is wel een
relatie tussen de betrouwbaarheid (hoe is gemeten) en de
construct-validiteit (wat is gemeten, zie
Hoofdstuk \@ref(ch:validiteit)), maar die twee begrippen zijn niet identiek.
Een voldoende betrouwbaarheid is een noodzakelijke, maar geen voldoende
voorwaarde voor validiteit. Anders gezegd: een toets die niet
betrouwbaar is kan ook niet valide zijn (want deze toets meet ook ruis),
maar een toets die wel betrouwbaar is hoeft nog niet valide te zijn.
Misschien meet de gebruikte toets wel heel betrouwbaar een ander
construct dan wat de bedoeling was.

Een instrument is construct-valide als het gemeten concept overeenkomt
met het bedoelde concept of construct. In
voorbeeld 12.3: de vragenlijst is valide als score uit de
vragenlijst overeenkomt met de kwaliteit van leven (wat dat dan ook moge
zijn) van de afasie-patiënten. Pas nadat aangetoond is dat een
instrument betrouwbaar is, heeft het zin om over de constructvaliditeit
van een meting te spreken. De betrouwbaarheid is een noodzakelijke maar
niet voldoende voorwaarde voor constructvaliditeit. Een onbetrouwbaar
instrument kan dus niet valide zijn, maar een betrouwbaar instrument
hoeft niet noodzakelijk valide te zijn.

Om de schrijfvaardigheid te meten, laten we de leerlingen een opstel
schrijven. We tellen het aantal letters *e* in elk opstel. Dat is een
zeer betrouwbare meting: verschillende beoordelaars komen tot hetzelfde
aantal *e*'s (beoordelaars zijn homogeen) en dezelfde beoordelaar levert
bij hetzelfde opstel ook steeds dezelfde uitkomst (beoordelaars zijn
stabiel). Het grote bezwaar hier is dat het aantal *e*'s in een opstel
niet of niet noodzakelijk overeenkomt met het concept
schrijfvaardigheid. Een leerling die meer *e*'s in zijn opstel verwerkt
is niet noodzakelijk een betere schrijver.

Onderzoekers weten weliswaar dat betrouwbaarheid een noodzakelijke maar
niet voldoende voorwaarde is voor de validiteit. Maar toch springen ze
niet altijd even zorgvuldig om met die begrippen. In vele onderzoeken
wordt er stilzwijgend van uit gegaan dat als de betrouwbaarheid
voldoende is, de validiteit dan ook gewaarborgd is. Ook in voorbeeld 12.3
wordt het onderscheid niet duidelijk gemaakt,
en bespreken de onderzoekers de construct-validiteit van hun nieuwe
vragenlijst niet expliciet.

## SPSS

Voor een betrouwbaarheidsanalyse van de $k=3$ oordelen over
spreekvaardigheid in
Tabel \@ref(tab:betrouwbaarheid):\

```
Analyze > Scale > Reliability Analysis...
```

Selecteer de variabelen die geacht worden hetzelfde construct te meten;
hier zijn dat de drie beoordelaars. We beschouwen deze $k=3$
beoordelaars als "items" die de eigenschap "spreekvaardigheid" meten van
10 studenten. Sleep deze variabelen naar het Variable(s) paneel.\
Vul als Scale label in een aanduiding van het construct, bijv.
`Spreekvaardigheid`.\
Kies als Method: `Alpha` voor Cronbach's Alpha (zie formule
\@ref(eq:cronbach-corr))\
Kies `Statistics…`, vink aan: Descriptives for
`Item, Scale, Scale if item deleted`, Inter-Item `Correlations`,
Summaries `Means, Variances`, en bevestig met `Continue` en daarna
nogmaals met `OK`.

De uitvoer bevat Cronbach's Alpha, de gevraagde
inter-item-correlaties (vooral hoog tussen beoordelaars 1 en 2), en (in
tabel Item-Total Statistics) de betrouwbaarheid indien we een bepaalde
beoordelaar zouden verwijderen. Deze laatste uitvoer leert ons dat
beoordelaars 1 en 2 van groter belang zijn dan beoordelaar 3. Als we
beoordelaar 1 of 2 zouden verwijderen dan stort de betrouwbaarheid in,
maar als we beoordelaar 3 zouden verwijderen dan neemt de
betrouwbaarheid zelfs toe (van 0.68 naar 0.84). Vermoedelijk heeft deze
beoordelaar een ander construct beoordeeld dan de anderen.

## R

Voor een betrouwbaarheidsanalyse van de $k=3$ oordelen over
spreekvaardigheid in Tabel \@ref(tab:betrouwbaarheid):\

```{r}
beoordelaars <- read.table(file="data/beoordelaars.txt", header=TRUE)
if (require(psych)) { # voor psych::alpha
  alpha( beoordelaars[,2:4] ) # kolommen 2 t/m 4
}
```

Deze uitvoer bevat Cronbach's Alpha (`raw_alpha 0.68`), en de
betrouwbaarheid indien we een bepaalde beoordelaar zouden verwijderen.
Als we beoordelaar 3 zouden verwijderen dan neemt de betrouwbaarheid
zelfs toe (van 0.68 naar 0.84). Over alle drie de beoordelaars is
`average_r=0.41`.

Correlaties tussen de $k$ beoordelaars of items worden niet expliciet
meegeleverd (al zijn ze wel af te leiden uit bovenstaande uitvoer), dus
vragen we die nog op:

```{r}
cor( beoordelaars[ ,c("B1","B2","B3") ] ) # expliciete kolomnamen
```

---

[^fn12-1]: $s^2_{(t+e)} = s^2_t + s^2_e + 2  r_{(t,e)} s_t s_e$, met hier $r_{(t,e)}=0$ volgens formule \@ref(eq:r-true-error).

[^fn12-2]: Een uitzondering hierop vormt de situatie indien $s^2_x=0$, en dus $s^2_t=0$, ergo betrouwbaarheid $\rho=0$; de afhankelijke variabele $x$ is dan niet goed geoperationaliseerd.

[^fn12-3]: In ons voorbeeld zijn er slechts $k=2$ beoordelaars, dus is er slechts één correlatie, en $\overline{r} = r_{1,2} = 0.75$.

[^fn12-4]: De zgn. 'intra-class correlation coefficient' (ICC) voor $k$ vaste beoordelaars of items is eveneens identiek aan de Cronbach's Alpha.
